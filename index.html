<!doctype html>
<html>

<head>
  <title>American Sign Language recognition</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="banner" style="background: url('img/hello.jpg') no-repeat center; background-size: cover; height: 200px;"></div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">What problem are we trying to solve?</h2>
            <p class="text">
                Speech-to-text software exists for many languages around the world and 
                enables us to more-easily communicate with others in the era of digital communication. 
                However, there is a lack of similar technologies for Sign Language. We plan to take a 
                step in solving this by creating a pipeline that detects and identifies the letters of 
                the alphabet in American Sign Language (ASL).
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <img class="image" src="img/cover.png">
          </div>
          <div class="flex-item flex-column">
            <p class="text text-large">
              Sparsh Binjrajka, sbinj (at) cs.washington.edu<br><br>
              Duncan Du, wenyudu (at) cs.washington.edu<br><br>
              Aldrich Fan, longxf (at) uw.edu<br><br>
              Josh Ning, long2000 (at) cs.washington.edu<br><br>
              <a target="_blank" href="http://cs.washington.edu/">Paul G. Allen School<br>of Computer Science & Engineering</a><br>
              <a target="_blank" href="http://www.washington.edu/">University of Washington</a><br><br>
              185 E Stevens Way NE<br>
              Seattle, WA 98195-2350
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <p class="text add-top-margin">
                We are creating a machine learning pipeline that detects and identifies the 26 letters of the ASL alphabet 
                in a video or webcam format. After exploring existing work on this topic, we found a model that showed very 
                good results. However, upon further investigation, we were uncertain if the dataset used was trustworthy. 
                The training and testing dataset were taken in the same setting and from similar angles, with most were taken 
                from the same hand. The test result was likely biased and not representative of how the model actually performs. 
                This was further confirmed when we tested the exact model out and it generated poor results. As such, we want to 
                create our own dataset to validate and test against the existing model. 
            </p>
          </div>
        </div>
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text Only-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Existing works</h2>
            <hr>
            <p class="text">
                There is an existing publication on identifying ASL Alphabets using the YOLOv5 model. 
                YOLO by ultralytics uses anchor-free detection and new convolutional layers to make 
                predictions faster and more accurate. The dataset the author used was small and is 
                mostly composed of pictures of his own hands. The model still got really good results 
                despite the constraints. Some limitations the authors pointed out was the distance of 
                the hand gesture to the camera has an impact on the identification and the background 
                noise does too. 
            </p>
          </div>
        </div>

        <!--End Text with Images and Image buttons-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text around Image-->

    <div class="content">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Text with Images and Videos</h2>
            <hr>
            <p class="text">
                In exploring existing work on ASL letter detection, we found a <a target="_blank" href="javascript:void(0)">pre-trained YOLOv5 model by David Lee</a>.
                A video demonstrating the results of the model shows almost all signs being detected with a confidence of at least 0.8.
            </p>
          </div>
        </div>
        <!-- <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image" src="img/dummay-img.png">
          </div>
          <div class="flex-item flex-item-stretch flex-column">
            <p class="text">
                We decided to replicate this model by downloading its dataset and, using a 70-20-10 split, trained the YOLOv8 model. 
                The results were quite underwhelming. It didn't recognise most signs correctly and for the ones it was correct, the 
                confidence was around 65%. This is not surprising since the dataset includes signs from the signer's perspective and 
                not from the viewer's perspective. Further, all of the pictures are of David Lee and his hand in the same environment. 
                Thus, the model probably is biased. 
            </p>
          </div>
        </div> -->
        <div class="flex-row">
            <div class="flex-item flex-item-stretch-2 flex-column">
              <p class="text">
                We decided to replicate this model by downloading its dataset and, using a 70-20-10 split, trained the YOLOv8 model. 
                The results were quite underwhelming. It didn’t recognise most signs correctly and for the ones it was correct, the 
                confidence was around 65%. This is not surprising since the dataset includes signs from the signer’s perspective and 
                not from the viewer’s perspective. Further, all of the pictures are of David Lee and his hand in the same environment. 
                Thus, the model probably is biased. 
              </p>
            </div>
            <div class="flex-item flex-item-stretch flex-column">
              <video preload controls autoplay loop muted playsinline class="image">
                <source src="vid/mov_bbb.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
        </div>
        <div class="flex-row">
            <div class="flex-item flex-column">
              <p class="text">
                The limitations of the previous dataset was that it did not include different signers, some of the training data was either incorrectly signed or the pictures were from the signer’s perspective, and the same background was used for all pictures. 
              </p>
            </div>
        </div>
        <div class="flex-row">
            <div class="flex-item flex-column">
              <p class="text">
                We decided then to look for youtube videos that we could then break it down to a series of frames (using 2fps) and then annotate those to create our dataset. 
                The idea behind this was to create a dataset that not only had correct signs from the viewer’s 
                perspective  but also a variety of signers with varying backgrounds which would make our model more accurate and less biased.
              </p>
            </div>
        </div>
        <div class="flex-row">
            <div class="flex-item flex-column">
              <p class="text">
                When we trained this model, we also included augmentations for horizontal flip (to include left handed signers), rotation and shearing by +- 5 degrees to make the prediction invariant to minute changes in the hand position. 
              </p>
            </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <video preload controls autoplay loop muted playsinline class="image">
              <source src="vid/mov_bbb.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="flex-item flex-item-stretch flex-column">
            <p class="text">
                This new model gave superior results to the previous model but it has its limitations as well (as can be seen in the video). 
                The model does very well in identifying signs if they’re placed a bit further away from the camera than if they’re placed closer. 
                This result made sense when we looked at our dataset again and most of the dataset were pictures that resembled the setup that 
                Duncan had with the webcam, i.e. located a bit further away from the webcam. Hence, the signs made by Duncan were more accurately 
                identified than the signs made by Sparsh who was located closer to the webcam. Another limitation of this model was that we could 
                not find very clear and sharp images for many of the signs since a lot of the frames we extracted from the videos either included 
                much of the signer’s face within the ROI box or their signs wer 
            </p>
          </div>
        </div>
        <p class="text">
            We created our own dataset was to generate pictures that had the following features:
        </p>
        <ol class="nested">
        <li>Correct signs and from the perspective of the viewer.</li>
        <li>Different signers in varying lighting and background settings.</li>
        <li>A mix of pictures that are close to the camera as well as further away.</li>
        </ol>
        <div class="flex-row">
            <div class="flex-item flex-item-stretch flex-column">
                <video preload controls autoplay loop muted playsinline class="image">
                <source src="vid/mov_bbb.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
            </div>
        
          <div class="flex-item flex-column">
            <p class="text">
                The idea behind this was to create a “testing” dataset that could be used to validate any model that has 
                good training results. We also used our dataset to train the YOLOv8 model and, as you can see from the video, 
                it does well for signs that are at a varying distance from the camera and is pretty accurate across all categories. 
                There are a few that don’t quite work well like the letters “M” and “N” but that can be improved.
            </p>
          </div>
        </div>
      </div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Credits</h2>
            <p class="text add-bottom-margin-large">
              Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel
              augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget
              condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit
              vel, luctus pulvinar, hendrerit id, lorem.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</body>

</html>